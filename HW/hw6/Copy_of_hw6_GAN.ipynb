{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "Y7y4wyYdEABR",
        "2NFjuZTPDxLn"
      ],
      "private_outputs": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oZ-C2Dgetg37"
      },
      "source": [
        "# **Homework 6 - Generative Adversarial Network**\n",
        "\n",
        "This is the example code of homework 6 of the machine learning course by Prof. Hung-yi Lee.\n",
        "\n",
        "\n",
        "In this homework, you are required to build a generative adversarial  network for anime face generation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JTBkY5QFf3QM"
      },
      "source": [
        "## Set up the environment\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y7y4wyYdEABR"
      },
      "source": [
        "### Packages Installation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6IQB485dD_eL"
      },
      "source": [
        "# Training progress bar\n",
        "!pip install -q qqdm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2NFjuZTPDxLn"
      },
      "source": [
        "### Download the dataset\n",
        "**Please use the link according to the last digit of your student ID first!**\n",
        "\n",
        "If all download links fail, please follow [here](https://drive.google.com/drive/folders/13T0Pa_WGgQxNkqZk781qhc5T9-zfh19e).\n",
        "\n",
        "* To open the file using your browser, use the link below (replace the id first!):\n",
        "https://drive.google.com/file/d/REPLACE_WITH_ID\n",
        "* e.g. https://drive.google.com/file/d/1IGrTr308mGAaCKotpkkm8wTKlWs9Jq-p"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uZomvVA2f607"
      },
      "source": [
        "# !gdown --id 1IGrTr308mGAaCKotpkkm8wTKlWs9Jq-p --output \"crypko_data.zip\"\n",
        "\n",
        "# Other download links\n",
        "#   Please uncomment the line according to the last digit of your student ID first\n",
        "\n",
        "# 0\n",
        "# !gdown --id 131zPaVoi-U--XThvzgRfaxrumc3YSBd3 --output \"{workspace_dir}/crypko_data.zip\"\n",
        "\n",
        "# 1\n",
        "# !gdown --id 1kCuIj1Pf3T2O94H9bUBxjPBKb---WOmH --output \"{workspace_dir}/crypko_data.zip\"\n",
        "\n",
        "# 2\n",
        "# !gdown --id 1boEoiiqBJwoHVvjmI0xgoutE9G0Rv8CD --output \"{workspace_dir}/crypko_data.zip\"\n",
        "\n",
        "# 3\n",
        "# !gdown --id 1Ic0mktAQQvnNAnswrPHsg-u2OWGBXTWF --output \"{workspace_dir}/crypko_data.zip\"\n",
        "\n",
        "# 4\n",
        "# !gdown --id 1PFcc25r9tLE7OyQ-CDadtysNdWizk6Yg --output \"{workspace_dir}/crypko_data.zip\"\n",
        "\n",
        "# 5\n",
        "# !gdown --id 1wgkrYkTrhwDSMdWa5NwpXeE4-7JaUuX2 --output \"{workspace_dir}/crypko_data.zip\"\n",
        "\n",
        "# 6\n",
        "# !gdown --id 19gwNYWi9gN9xVL86jC3v8qqNtrXyq5Bf --output \"{workspace_dir}/crypko_data.zip\"\n",
        "\n",
        "# 7 \n",
        "# !gdown --id 1-KPZB6frRSRLRAtQfafKCVA7em0_NrJG --output \"{workspace_dir}/crypko_data.zip\"\n",
        "\n",
        "# 8\n",
        "# !gdown --id 1rNBfmn0YBzXuG5ub7CXbsGwduZqEs8hx --output \"{workspace_dir}/crypko_data.zip\"\n",
        "\n",
        "# 9\n",
        "# !gdown --id 113NEISX-2j6rBd1yyBx0c3_9nPIzSNz- --output \"{workspace_dir}/crypko_data.zip\"\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pNtT1WCOyRNt"
      },
      "source": [
        "###Unzip the downloaded file.\n",
        "The unzipped tree structure is like \n",
        "```\n",
        "faces/\n",
        "├── 1.jpg\n",
        "├── 2.jpg\n",
        "├── 3.jpg\n",
        "...\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s2qR-0hjqWE6"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "# You may replace the workspace directory if you want.\n",
        "workspace_dir = '.'\n",
        "\n",
        "#!unzip -q \"/content/drive/MyDrive/ML2021/hw6/crypko_data.zip\" -d \"{workspace_dir}/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NjfM46dtmxXj"
      },
      "source": [
        "## Random seed\n",
        "Set the random seed to a certain value for reproducibility."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OWuecW1imz42"
      },
      "source": [
        "import random\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def same_seeds(seed):\n",
        "    # Python built-in random module\n",
        "    random.seed(seed)\n",
        "    # Numpy\n",
        "    np.random.seed(seed)\n",
        "    # Torch\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "same_seeds(2021)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uCTPz2iRQmwe"
      },
      "source": [
        "## Import Packages\n",
        "First, we need to import packages that will be used later.\n",
        "\n",
        "Like hw3, we highly rely on **torchvision**, a library of PyTorch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TC8RRsX0QhL-"
      },
      "source": [
        "import os\n",
        "import glob\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch import optim\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "from qqdm.notebook import qqdm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kYjZ_G83_YX4"
      },
      "source": [
        "## Dataset\n",
        "1. Resize the images to (64, 64)\n",
        "1. Linearly map the values from [0, 1] to  [-1, 1].\n",
        "\n",
        "Please refer to [PyTorch official website](https://pytorch.org/vision/stable/transforms.html) for details about different transforms.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UZ6d0_cr8R26"
      },
      "source": [
        "class CrypkoDataset(Dataset):\n",
        "    def __init__(self, fnames, transform):\n",
        "        self.transform = transform\n",
        "        self.fnames = fnames\n",
        "        self.num_samples = len(self.fnames)\n",
        "\n",
        "    def __getitem__(self,idx):\n",
        "        fname = self.fnames[idx]\n",
        "        # 1. Load the image\n",
        "        img = torchvision.io.read_image(fname)\n",
        "        # 2. Resize and normalize the images using torchvision.\n",
        "        img = self.transform(img)\n",
        "        return img\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_samples\n",
        "\n",
        "\n",
        "def get_dataset(root):\n",
        "    fnames = glob.glob(os.path.join(root, '*'))\n",
        "    # 1. Resize the image to (64, 64)\n",
        "    # 2. Linearly map [0, 1] to [-1, 1]\n",
        "    compose = [\n",
        "        transforms.ToPILImage(),\n",
        "        transforms.Resize((64, 64)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)),\n",
        "    ]\n",
        "    transform = transforms.Compose(compose)\n",
        "    dataset = CrypkoDataset(fnames, transform)\n",
        "    return dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tGwdVhOKSjLY"
      },
      "source": [
        "### Show some images\n",
        "Note that the values are in the range of [-1, 1], we should shift them to the valid range, [0, 1], to display correctly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "34mVNtHn7cwF"
      },
      "source": [
        "dataset = get_dataset(os.path.join(workspace_dir, 'faces'))\n",
        "\n",
        "images = [dataset[i] for i in range(16)]\n",
        "grid_img = torchvision.utils.make_grid(images, nrow=4)\n",
        "plt.figure(figsize=(10,10))\n",
        "plt.imshow(grid_img.permute(1, 2, 0))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nhxUjRUuHdti"
      },
      "source": [
        "images = [(dataset[i]+1)/2 for i in range(16)]\n",
        "grid_img = torchvision.utils.make_grid(images, nrow=4)\n",
        "plt.figure(figsize=(10,10))\n",
        "plt.imshow(grid_img.permute(1, 2, 0))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XkvZ4JgCHCZD"
      },
      "source": [
        "## Model\n",
        "Here, we use DCGAN as the model structure. Feel free to modify your own model structure.\n",
        "\n",
        "Note that the `N` of the input/output shape stands for the batch size."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F0I1jRd6HFmm"
      },
      "source": [
        "def weights_init(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Conv') != -1:\n",
        "        m.weight.data.normal_(0.0, 0.02)\n",
        "    elif classname.find('BatchNorm') != -1:\n",
        "        m.weight.data.normal_(1.0, 0.02)\n",
        "        m.bias.data.fill_(0)\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    \"\"\"\n",
        "    Input shape: (N, in_dim)\n",
        "    Output shape: (N, 3, 64, 64)\n",
        "    \"\"\"\n",
        "    def __init__(self, in_dim, dim=64):\n",
        "        super(Generator, self).__init__()\n",
        "        def dconv_bn_relu(in_dim, out_dim):\n",
        "            return nn.Sequential(\n",
        "                nn.ConvTranspose2d(in_dim, out_dim, 5, 2,\n",
        "                                   padding=2, output_padding=1, bias=False),\n",
        "                nn.BatchNorm2d(out_dim),\n",
        "                nn.ReLU()\n",
        "            )\n",
        "        self.l1 = nn.Sequential(\n",
        "            nn.Linear(in_dim, dim * 8 * 4 * 4, bias=False),\n",
        "            nn.BatchNorm1d(dim * 8 * 4 * 4),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.l2_5 = nn.Sequential(\n",
        "            dconv_bn_relu(dim * 8, dim * 4),\n",
        "            dconv_bn_relu(dim * 4, dim * 2),\n",
        "            dconv_bn_relu(dim * 2, dim),\n",
        "            nn.ConvTranspose2d(dim, 3, 5, 2, padding=2, output_padding=1),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "        self.apply(weights_init)\n",
        "\n",
        "    def forward(self, x):\n",
        "        y = self.l1(x)\n",
        "        y = y.view(y.size(0), -1, 4, 4)\n",
        "        y = self.l2_5(y)\n",
        "        return y\n",
        "\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    \"\"\"\n",
        "    Input shape: (N, 3, 64, 64)\n",
        "    Output shape: (N, )\n",
        "    \"\"\"\n",
        "    def __init__(self, in_dim, dim=64):\n",
        "        super(Discriminator, self).__init__()\n",
        "\n",
        "        def conv_bn_lrelu(in_dim, out_dim):\n",
        "            return nn.Sequential(\n",
        "                nn.Conv2d(in_dim, out_dim, 5, 2, 2),\n",
        "                nn.BatchNorm2d(out_dim),\n",
        "                nn.LeakyReLU(0.2),\n",
        "            )\n",
        "            \n",
        "        \"\"\" Medium: Remove the last sigmoid layer for WGAN. \"\"\"\n",
        "        self.ls = nn.Sequential(\n",
        "            nn.Conv2d(in_dim, dim, 5, 2, 2), \n",
        "            nn.LeakyReLU(0.2),\n",
        "            conv_bn_lrelu(dim, dim * 2),\n",
        "            conv_bn_lrelu(dim * 2, dim * 4),\n",
        "            conv_bn_lrelu(dim * 4, dim * 8),\n",
        "            nn.Conv2d(dim * 8, 1, 4),\n",
        "            nn.Sigmoid(), \n",
        "        )\n",
        "        self.apply(weights_init)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        y = self.ls(x)\n",
        "        y = y.view(-1)\n",
        "        return y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cxo4teqaO5RJ"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v5sCGIUtSViC"
      },
      "source": [
        "### Initialization\n",
        "- hyperparameters\n",
        "- model\n",
        "- optimizer\n",
        "- dataloader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2EqomOouHezf"
      },
      "source": [
        "# Training hyperparameters\n",
        "batch_size = 64\n",
        "z_dim = 100\n",
        "z_sample = Variable(torch.randn(100, z_dim)).cuda()\n",
        "lr = 1e-4\n",
        "\n",
        "\"\"\" Medium: WGAN, 50 epoch, n_critic=5, clip_value=0.01 \"\"\"\n",
        "n_epoch = 1 # 50\n",
        "n_critic = 1 # 5\n",
        "# clip_value = 0.01\n",
        "\n",
        "log_dir = os.path.join(workspace_dir, 'logs')\n",
        "ckpt_dir = os.path.join(workspace_dir, 'checkpoints')\n",
        "os.makedirs(log_dir, exist_ok=True)\n",
        "os.makedirs(ckpt_dir, exist_ok=True)\n",
        "\n",
        "# Model\n",
        "G = Generator(in_dim=z_dim).cuda()\n",
        "D = Discriminator(3).cuda()\n",
        "G.train()\n",
        "D.train()\n",
        "\n",
        "# Loss\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "\"\"\" Medium: Use RMSprop for WGAN. \"\"\"\n",
        "# Optimizer\n",
        "opt_D = torch.optim.Adam(D.parameters(), lr=lr, betas=(0.5, 0.999))\n",
        "opt_G = torch.optim.Adam(G.parameters(), lr=lr, betas=(0.5, 0.999))\n",
        "# opt_D = torch.optim.RMSprop(D.parameters(), lr=lr)\n",
        "# opt_G = torch.optim.RMSprop(G.parameters(), lr=lr)\n",
        "\n",
        "\n",
        "# DataLoader\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vpJA1wzi0tii"
      },
      "source": [
        "### Training loop\n",
        "We store some pictures regularly to monitor the current \bperformance of the Generator, and regularly record checkpoints."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dgkqPih1o5Az"
      },
      "source": [
        "steps = 0\n",
        "for e, epoch in enumerate(range(n_epoch)):\n",
        "    progress_bar = qqdm(dataloader)\n",
        "    for i, data in enumerate(progress_bar):\n",
        "        imgs = data\n",
        "        imgs = imgs.cuda()\n",
        "\n",
        "        bs = imgs.size(0)\n",
        "\n",
        "        # ============================================\n",
        "        #  Train D\n",
        "        # ============================================\n",
        "        z = Variable(torch.randn(bs, z_dim)).cuda()\n",
        "        r_imgs = Variable(imgs).cuda()\n",
        "        f_imgs = G(z)\n",
        "\n",
        "        \"\"\" Medium: Use WGAN Loss. \"\"\"\n",
        "        # Label\n",
        "        r_label = torch.ones((bs)).cuda()\n",
        "        f_label = torch.zeros((bs)).cuda()\n",
        "\n",
        "        # Model forwarding\n",
        "        r_logit = D(r_imgs.detach())\n",
        "        f_logit = D(f_imgs.detach())\n",
        "        \n",
        "        # Compute the loss for the discriminator.\n",
        "        r_loss = criterion(r_logit, r_label)\n",
        "        f_loss = criterion(f_logit, f_label)\n",
        "        loss_D = (r_loss + f_loss) / 2\n",
        "\n",
        "        # WGAN Loss\n",
        "        # loss_D = -torch.mean(D(r_imgs)) + torch.mean(D(f_imgs))\n",
        "       \n",
        "\n",
        "        # Model backwarding\n",
        "        D.zero_grad()\n",
        "        loss_D.backward()\n",
        "\n",
        "        # Update the discriminator.\n",
        "        opt_D.step()\n",
        "\n",
        "        \"\"\" Medium: Clip weights of discriminator. \"\"\"\n",
        "        # for p in D.parameters():\n",
        "        #    p.data.clamp_(-clip_value, clip_value)\n",
        "\n",
        "        # ============================================\n",
        "        #  Train G\n",
        "        # ============================================\n",
        "        if steps % n_critic == 0:\n",
        "            # Generate some fake images.\n",
        "            z = Variable(torch.randn(bs, z_dim)).cuda()\n",
        "            f_imgs = G(z)\n",
        "\n",
        "            # Model forwarding\n",
        "            f_logit = D(f_imgs)\n",
        "            \n",
        "            \"\"\" Medium: Use WGAN Loss\"\"\"\n",
        "            # Compute the loss for the generator.\n",
        "            loss_G = criterion(f_logit, r_label)\n",
        "            # WGAN Loss\n",
        "            # loss_G = -torch.mean(D(f_imgs))\n",
        "\n",
        "            # Model backwarding\n",
        "            G.zero_grad()\n",
        "            loss_G.backward()\n",
        "\n",
        "            # Update the generator.\n",
        "            opt_G.step()\n",
        "\n",
        "        steps += 1\n",
        "        \n",
        "        # Set the info of the progress bar\n",
        "        #   Note that the value of the GAN loss is not directly related to\n",
        "        #   the quality of the generated images.\n",
        "        progress_bar.set_infos({\n",
        "            'Loss_D': round(loss_D.item(), 4),\n",
        "            'Loss_G': round(loss_G.item(), 4),\n",
        "            'Epoch': e+1,\n",
        "            'Step': steps,\n",
        "        })\n",
        "\n",
        "    G.eval()\n",
        "    f_imgs_sample = (G(z_sample).data + 1) / 2.0\n",
        "    filename = os.path.join(log_dir, f'Epoch_{epoch+1:03d}.jpg')\n",
        "    torchvision.utils.save_image(f_imgs_sample, filename, nrow=10)\n",
        "    print(f' | Save some samples to {filename}.')\n",
        "    \n",
        "    # Show generated images in the jupyter notebook.\n",
        "    grid_img = torchvision.utils.make_grid(f_imgs_sample.cpu(), nrow=10)\n",
        "    plt.figure(figsize=(10,10))\n",
        "    plt.imshow(grid_img.permute(1, 2, 0))\n",
        "    plt.show()\n",
        "    G.train()\n",
        "\n",
        "    if (e+1) % 5 == 0 or e == 0:\n",
        "        # Save the checkpoints.\n",
        "        torch.save(G.state_dict(), os.path.join(ckpt_dir, 'G.pth'))\n",
        "        torch.save(D.state_dict(), os.path.join(ckpt_dir, 'D.pth'))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e2uJFmTtKBeH"
      },
      "source": [
        "## Inference\n",
        "Use the trained model to generate anime faces!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tXPXcVD_HJB2"
      },
      "source": [
        "### Load model "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4JnQdNx2SUS2"
      },
      "source": [
        "import torch\n",
        "\n",
        "G = Generator(z_dim)\n",
        "G.load_state_dict(torch.load(os.path.join(ckpt_dir, 'G.pth')))\n",
        "G.eval()\n",
        "G.cuda()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-I8PDocbHQiN"
      },
      "source": [
        "### Generate and show some images.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x-SYKrRea_-Q"
      },
      "source": [
        "# Generate 1000 images and make a grid to save them.\n",
        "n_output = 1000\n",
        "z_sample = Variable(torch.randn(n_output, z_dim)).cuda()\n",
        "imgs_sample = (G(z_sample).data + 1) / 2.0\n",
        "log_dir = os.path.join(workspace_dir, 'logs')\n",
        "filename = os.path.join(log_dir, 'result.jpg')\n",
        "torchvision.utils.save_image(imgs_sample, filename, nrow=10)\n",
        "\n",
        "# Show 32 of the images.\n",
        "grid_img = torchvision.utils.make_grid(imgs_sample[:32].cpu(), nrow=10)\n",
        "plt.figure(figsize=(10,10))\n",
        "plt.imshow(grid_img.permute(1, 2, 0))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6B04ATOTHc4F"
      },
      "source": [
        "### Compress the generated images using **tar**.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mbcmoTQpz_yf"
      },
      "source": [
        "# Save the generated images.\n",
        "os.makedirs('output', exist_ok=True)\n",
        "for i in range(1000):\n",
        "    torchvision.utils.save_image(imgs_sample[i], f'output/{i+1}.jpg')\n",
        "  \n",
        "# Compress the images.\n",
        "%cd output\n",
        "!tar -zcf ../images.tgz *.jpg\n",
        "%cd .."
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}